{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TextRank import TextRank\n",
    "import numpy\n",
    "import nltk\n",
    "import requests\n",
    "import os \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.join(os.path.dirname('__file__'), '..', ) + '/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_url = 'https://www.eea.europa.eu/airs/2017/resource-efficiency-and-low-carbon-economy/environmental-protection-expenditure'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide the base url: https://www.eea.europa.eu/airs/2017/resource-efficiency-and-low-carbon-economy/environmental-protection-expenditure\n",
      "There are 18 links to be scraped\n",
      "Please provide project name: eu_waste\n",
      "18\n",
      "Invalid URL '#content': No schema supplied. Perhaps you meant http://#content?\n",
      "Done with 340\n",
      "Done with 341\n",
      "Done with 342\n",
      "Done with 343\n",
      "Done with 344\n",
      "Done with 345\n",
      "Done with 346\n",
      "Done with 347\n",
      "Done with 348\n",
      "Done with 349\n",
      "Done with 350\n",
      "Done with 351\n",
      "Done with 352\n",
      "Done with 353\n",
      "Done with 354\n",
      "Done with 355\n",
      "Done with 356\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "from TextRank import TextRank\n",
    "import numpy\n",
    "import nltk\n",
    "import requests\n",
    "import os \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "output_path = os.path.join(os.path.dirname('__file__'), '..', ) + '/output/'\n",
    "if os.path.exists(output_path):\n",
    "    pass\n",
    "else:\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "class Scrape_Page(object):\n",
    "    '''The object for the scraped page\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def _random_sleep(minimum=3, maximum=10, sd=1):\n",
    "        \"\"\"\n",
    "        sets a random time to sleep between requests, drawing random values from an exponential distribution\n",
    "        if no parameters are set, the values [3, 10, 1] have been pre-specified\n",
    "\n",
    "        :param minimum: the min time of the timer <float>\n",
    "        :param maximum: the max time of the timer <float>\n",
    "        :param sd: the standard deviation <float>\n",
    "        :return: time.sleep( value drawn )\n",
    "        \"\"\"\n",
    "        value = minimum + sd * np.random.exponential()\n",
    "        if value > maximum:\n",
    "            value = maximum + np.random.uniform(0, 1)\n",
    "        time.sleep(value)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _requesting(url, timeout = 10, parser = 'lxml'):\n",
    "        '''\n",
    "        Makes the request to the given url, escaopes after specified timeout.\n",
    "        Returns BeautifulSoup object.\n",
    "        :param url: The url to investigate\n",
    "        :param timeout: timeout of the request\n",
    "        :param parser: The parser used to create the soup object \n",
    "        '''\n",
    "        response = requests.get(url, timeout = timeout)\n",
    "        html = response.content\n",
    "        response.close()\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        return(soup)\n",
    "    \n",
    "    @classmethod\n",
    "    def return_text(cls, url):\n",
    "        '''\n",
    "        Returns the text part as one string from the page\n",
    "        :param url: The page url to be scraped\n",
    "        '''\n",
    "        soup = cls._requesting(url)\n",
    "        p_list = soup.findAll('p')\n",
    "        whole_text = '{}\\n'.format(url)\n",
    "        for tag in p_list:\n",
    "            whole_text += tag.text\n",
    "        return(whole_text)\n",
    "    \n",
    "    @classmethod\n",
    "    def return_links(cls, url, return_internal=True, only_from_tag = None):\n",
    "        '''\n",
    "        Returns the links in the give page.\n",
    "        :param url: the page on which we are looking for links\n",
    "        :param return_internal: return Links that are from the same mother domain?\n",
    "        '''\n",
    "        if only_from_tag != None:\n",
    "            soup = cls._requesting(url)\n",
    "            tags = soup.findAll(only_from_tag)\n",
    "            link_list = []\n",
    "            for tag in tags:\n",
    "                if tag.find('a'):\n",
    "                    link_list.append(tag.find('a').get('href'))\n",
    "            return link_list\n",
    "        else:\n",
    "            soup = cls._requesting(url)\n",
    "            all_a = soup.findAll('a')\n",
    "            link_list = []\n",
    "            for tag in all_a:\n",
    "                try:\n",
    "                    link_list.append(tag.get('href'))\n",
    "                except:\n",
    "                    pass\n",
    "            if return_internal == False:\n",
    "                home_url = urlparse(url)[1]\n",
    "\n",
    "\n",
    "                for sub_link in link_list:\n",
    "                    try:\n",
    "                        if home_url == urlparse(sub_link)[1]:\n",
    "                            link_list.remove(sub_link)\n",
    "                            print(sub_link)\n",
    "\n",
    "                    except Exception as e:\n",
    "\n",
    "                        print(e)\n",
    "                return link_list\n",
    "\n",
    "\n",
    "            else:\n",
    "                return link_list\n",
    "    @classmethod\n",
    "    def save_pages_text(cls, url, return_internal = False, project_name = None):\n",
    "        '''\n",
    "        Uses return_links to save the p tags of the html body\n",
    "        :param: return_internal if true, it returns also links that are pointing to the same page\n",
    "        :param: project_name is the name used for the folder\n",
    "        '''\n",
    "        link_list = Scrape_Page.return_links(url, only_from_tag= 'p', return_internal=return_internal)\n",
    "        link_list = list(set(link_list))\n",
    "        print('There are {} links to be scraped'.format(len(link_list)))\n",
    "        if project_name!= None:\n",
    "            pass\n",
    "        else:\n",
    "            project_name = input('Please provide project name: ')\n",
    "        project_folder_name = output_path + '/{}/'.format(project_name)\n",
    "        project_raw_data = project_folder_name + '/raw_data/'\n",
    "        if not os.path.exists(project_folder_name):\n",
    "            os.makedirs(project_folder_name)\n",
    "            os.makedirs(project_raw_data)\n",
    "        else:\n",
    "            if not os.path.exists(project_raw_data):\n",
    "                os.makedirs(project_raw_data)\n",
    "        print(len(link_list))\n",
    "        for l in link_list:\n",
    "            try:    \n",
    "                text = Scrape_Page.return_text(url=l)\n",
    "                save_file = open(project_raw_data + '{}.txt'.format(len(os.listdir(project_raw_data))), 'w')\n",
    "                save_file.write('{}'.format(text))\n",
    "                save_file.close()\n",
    "                cls._random_sleep()\n",
    "                print('Done with {}'.format(len(os.listdir(project_raw_data))))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        print('Finished')\n",
    "        \n",
    "home_url = input('Please provide the base url: ')\n",
    "Scrape_Page.save_pages_text(home_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Scrape_Page.return_links(url=home_url, only_from_tag='p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 129 links to be scraped\n",
      "Please provide project name: eu_waste\n",
      "Done with 2\n",
      "Invalid URL '#content': No schema supplied. Perhaps you meant http://#content?\n",
      "Done with 3\n",
      "Done with 4\n",
      "Done with 5\n",
      "Done with 6\n",
      "Done with 7\n",
      "Done with 8\n",
      "Done with 9\n",
      "Done with 10\n",
      "Done with 11\n",
      "Done with 12\n",
      "Done with 13\n",
      "Done with 14\n",
      "Done with 15\n",
      "Done with 16\n",
      "Done with 17\n",
      "Done with 18\n",
      "Done with 19\n",
      "Done with 20\n",
      "Done with 21\n",
      "Done with 22\n",
      "Done with 23\n",
      "Done with 24\n",
      "Done with 25\n",
      "Done with 26\n",
      "Done with 27\n",
      "Done with 28\n",
      "Done with 29\n",
      "Invalid URL '/data-and-maps/indicators/': No schema supplied. Perhaps you meant http:///data-and-maps/indicators/?\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a38ec439dcc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mScrape_Page\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pages_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhome_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-c7f2787ad4bf>\u001b[0m in \u001b[0;36msave_pages_text\u001b[0;34m(cls, url, return_internal, project_name)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mScrape_Page\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_sleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0msave_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_raw_data\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'{}.txt'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_raw_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-c7f2787ad4bf>\u001b[0m in \u001b[0;36m_random_sleep\u001b[0;34m(minimum, maximum, sd)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaximum\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaximum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for l in link:\n",
    "    try:    \n",
    "        texts.append(Scraped_Page.return_text(l))\n",
    "        time.sleep(5)\n",
    "        for text in texts:\n",
    "            save_file = open(output_path + '{}.txt'.format(len(os.listdir(output_path))), 'w')\n",
    "            save_file.write('{}'.format(text))\n",
    "            save_file.close()\n",
    "            print('done')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = urlparse('https://www.eea.europa.eu/bg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
