{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = {\n",
    "    'webpage-1': set(['webpage-2', 'webpage-4', 'webpage-5', 'webpage-6', 'webpage-8', 'webpage-9', 'webpage-10']),\n",
    "    'webpage-2': set(['webpage-5', 'webpage-6']),\n",
    "    'webpage-3': set(['webpage-10']),\n",
    "    'webpage-4': set(['webpage-9']),\n",
    "    'webpage-5': set(['webpage-2', 'webpage-4']),\n",
    "    'webpage-6': set([]), # dangling page\n",
    "    'webpage-7': set(['webpage-1', 'webpage-3', 'webpage-4']),\n",
    "    'webpage-8': set(['webpage-1']),\n",
    "    'webpage-9': set(['webpage-1', 'webpage-2', 'webpage-3', 'webpage-8', 'webpage-10']),\n",
    "    'webpage-10': set(['webpage-2', 'webpage-3', 'webpage-8', 'webpage-9']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(links):\n",
    "    website_list = links.keys()\n",
    "    return {website : index for (index, website) in enumerate(website_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'webpage-2': 1, 'webpage-7': 2, 'webpage-5': 7, 'webpage-4': 3, 'webpage-9': 4, 'webpage-1': 5, 'webpage-10': 6, 'webpage-6': 0, 'webpage-3': 8, 'webpage-8': 9}\n"
     ]
    }
   ],
   "source": [
    "website_index = build_index(links)\n",
    "print(website_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transition_matrix(links, index):\n",
    "    total_links = 0\n",
    "    A = np.zeros((len(index), len(index)))\n",
    "    for webpage in links:\n",
    "        # dangling page\n",
    "        if not links[webpage]:\n",
    "            # assign equal prob to transition to all other pages\n",
    "            A[index[webpage]] = np.ones(len(index)) / len(index)\n",
    "        else:\n",
    "            for dest_webpage in links[webpage]:\n",
    "                total_links += 1\n",
    "                A[index[webpage]][index[dest_webpage]] = 1.0 / len(links[webpage])\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = build_transition_matrix(links, website_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "webpage-6\n",
      "webpage-2\n",
      "webpage-7\n",
      "webpage-4\n",
      "webpage-9\n",
      "webpage-1\n",
      "webpage-10\n",
      "webpage-5\n",
      "webpage-3\n",
      "webpage-8\n"
     ]
    }
   ],
   "source": [
    "for i in links:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'webpage-1': {'webpage-10',\n",
       "  'webpage-2',\n",
       "  'webpage-4',\n",
       "  'webpage-5',\n",
       "  'webpage-6',\n",
       "  'webpage-8',\n",
       "  'webpage-9'},\n",
       " 'webpage-10': {'webpage-2', 'webpage-3', 'webpage-8', 'webpage-9'},\n",
       " 'webpage-2': {'webpage-5', 'webpage-6'},\n",
       " 'webpage-3': {'webpage-10'},\n",
       " 'webpage-4': {'webpage-9'},\n",
       " 'webpage-5': {'webpage-2', 'webpage-4'},\n",
       " 'webpage-6': set(),\n",
       " 'webpage-7': {'webpage-1', 'webpage-3', 'webpage-4'},\n",
       " 'webpage-8': {'webpage-1'},\n",
       " 'webpage-9': {'webpage-1',\n",
       "  'webpage-10',\n",
       "  'webpage-2',\n",
       "  'webpage-3',\n",
       "  'webpage-8'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'webpage-1': 5,\n",
       " 'webpage-10': 6,\n",
       " 'webpage-2': 1,\n",
       " 'webpage-3': 8,\n",
       " 'webpage-4': 3,\n",
       " 'webpage-5': 7,\n",
       " 'webpage-6': 0,\n",
       " 'webpage-7': 2,\n",
       " 'webpage-8': 9,\n",
       " 'webpage-9': 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pageRank(A, eps=.0001, d=.85):\n",
    "    P = np.ones(len(A)) / len(A)\n",
    "    while True:\n",
    "        new_P = np.ones(len(A)) * (1 - d) / (len(A)) + d * A.T.dot(P)\n",
    "        delta = abs(new_P - P).sum()\n",
    "        if delta <= eps:\n",
    "            return new_P\n",
    "        else:\n",
    "            P = new_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pageRank(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 1, 5, 0, 7, 9, 3, 8, 2]\n"
     ]
    }
   ],
   "source": [
    "print([item[0] for item in sorted(enumerate(results), key=lambda item: -item[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'webpage-1': 5,\n",
       " 'webpage-10': 6,\n",
       " 'webpage-2': 1,\n",
       " 'webpage-3': 8,\n",
       " 'webpage-4': 3,\n",
       " 'webpage-5': 7,\n",
       " 'webpage-6': 0,\n",
       " 'webpage-7': 2,\n",
       " 'webpage-8': 9,\n",
       " 'webpage-9': 4}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.cluster.util import cosine_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    "    sent1 = [w.lower for w in sent1]\n",
    "    sent2 = [w.lower for w in sent2]\n",
    "    \n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    \n",
    "    # build the vecotr for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            vector1[all_words.index(w)] += 1\n",
    "    # build the vecotr for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            vector2[all_words.index(w)] += 1\n",
    "    \n",
    "    return cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print(sentence_similarity(\"This is a good sentence\".split(), \"This is a bad sentence\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = brown.sents('ca01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(sentences, stopwords=None):\n",
    "    # create an empty similarity matrix\n",
    "    S = np.zeros((len(sentences), len(sentences)))\n",
    "    \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "            else:\n",
    "                S[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    \n",
    "    # normalise the matrix row-wise\n",
    "    for idx in range(len(S)):\n",
    "        S[idx] /= S[idx].sum()\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = build_similarity_matrix(sentences, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textrank(sentences, top_n=5, stopwords=None):\n",
    "    \"\"\"\n",
    "    sentences = a list of sentences [[w11, w12, ...], [w21, w22, ...], ...]\n",
    "    top_n = how may sentences the summary should contain\n",
    "    stopwords = a list of stopwords\n",
    "    \"\"\"\n",
    "    S = build_similarity_matrix(sentences, stop_words) \n",
    "    sentence_ranks = pageRank(S)\n",
    " \n",
    "    # Sort the sentence ranks\n",
    "    ranked_sentence_indexes = [item[0] for item in sorted(enumerate(sentence_ranks), key=lambda item: -item[1])]\n",
    "    selected_sentences = sorted(ranked_sentence_indexes[:top_n])\n",
    "    summary = itemgetter(*selected_sentences)(sentences)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../pythonprojects/Standard_scraper/output/labeled_newspaper_articles.pickle', 'rb') as handle:\n",
    "    t = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "    #stems = []\n",
    "    #for item in tokens: \n",
    "    #    stems.append(PorterStemmer().stem(item))\n",
    "    #return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = t[206667280852270103159815161964730877912]['Text'].replace('.', '. ').replace('or register with your social accountAlready have an account? Log in', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Saudi Arabia promised to act decisively to keep oil prices under control, signaling a real supply boost approaching 1 million barrels a day is on the way to global markets.\n",
    "\n",
    "“We will do whatever is necessary to keep the market in balance,” Saudi Energy Minister Khalid Al-Falih told reporters on Saturday, while sitting alongside his Russian counterpart Alexander Novak at OPEC headquarters in Vienna. Consumers can rest assured that “their energy supplies are available, are being stewarded by a responsible group of producers.”\n",
    "\n",
    "After a last-minute compromise that overcame Iranian opposition, Friday’s OPEC agreement delivered a pledge for a “nominal” supply increase of 1 million barrels a day. In reality, several countries are unable to pump more so the real output boost would have been smaller -- ranging from Iran’s 500,000 barrel-a-day estimate up to Iraq’s prediction for as much as 800,000.\n",
    "\n",
    "Saturday’s agreement dropped the pledge that the 1 million barrel-a-day increase should be shared proportionally among members, opening the way for the full volume to flow, Al-Falih said.\n",
    "\n",
    "\"If we allocated the number pro-rata basis among the 24 countries, given the capacity of those countries that can increase, it had been estimated that about 60 percent will be achieved,” Al-Falih said. “But because we went away from allocation on a pro-rata basis, we will be closer to 1 million than to 600,000 barrels a day.\"\n",
    "\n",
    "The group’s communique still pledged a return to 100 percent compliance with the original 2016 agreement -- ending a period of deeper-than-intended cuts -- but Al-Falih insisted that no individual country will be subject to a strict output cap. That means nations including Saudi Arabia can fill the gap left by falling production elsewhere in the OPEC+ alliance.\n",
    "\n",
    "Al-Falih also said that a committee dominated by Saudi Arabia and Russia will take direct responsibility for overseeing the flow of additional oil, cementing the two nations’ dominance over a group that pumps more than half the world’s crude.\n",
    "\n",
    "This central role for the two chief proponents of increasing oil supplies to alleviate high prices could provide some assurance to traders, who spurred the biggest rally in U.S. crude futures in six months on Friday after the Organization of Petroleum Exporting Countries published a vaguely worded agreement that fell short of specific output pledges.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [tokenize(x) for x in sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. “ We will do whatever is necessary to keep the market in balance , ” Saudi Energy Minister Khalid Al-Falih told reporters on Saturday , while sitting alongside his Russian counterpart Alexander Novak at OPEC headquarters in Vienna .\n",
      "2. In reality , several countries are unable to pump more so the real output boost would have been smaller -- ranging from Iran ’ s 500,000 barrel-a-day estimate up to Iraq ’ s prediction for as much as 800,000 .\n",
      "3. `` If we allocated the number pro-rata basis among the 24 countries , given the capacity of those countries that can increase , it had been estimated that about 60 percent will be achieved , ” Al-Falih said .\n",
      "4. That means nations including Saudi Arabia can fill the gap left by falling production elsewhere in the OPEC+ alliance .\n",
      "5. This central role for the two chief proponents of increasing oil supplies to alleviate high prices could provide some assurance to traders , who spurred the biggest rally in U.S. crude futures in six months on Friday after the Organization of Petroleum Exporting Countries published a vaguely worded agreement that fell short of specific output pledges .\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(textrank(sentences, stopwords=stopwords.words('english'))):\n",
    "    print(\"%s. %s\" % ((idx + 1), ' '.join(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
